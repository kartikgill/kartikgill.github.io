<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kartik Chaudhary</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kartik Chaudhary</name>
              </p>
              <p>I am a Machine Learning Scientist at Google, where I work on designing AI based solutions/products leveraging the advancements in the field of Computer Vision, NLP and Machine Learning.
              </p>
              <p>
                I am passionate about learning and inventing new ways of improving ML and DL algorithms to solve real world problems more efficiently. Do checkout my blog on Artificial Intelligence - <a href="https://www.dropsofai.com">Drops of AI</a>
              </p>
              <p style="text-align:center">
                <a href="mailto:kartikgill96@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://in.linkedin.com/in/chaudharykartik">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=IixpFX4AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/kartikgill96">Twitter</a> &nbsp/&nbsp <br>
                <a href="https://github.com/kartikgill/">Github</a> &nbsp/&nbsp
                <a href="https://medium.com/@kartikgill96">Medium</a> &nbsp/&nbsp
                <a href="https://dropsofai.com">Blog</a> 

              </p>
                  <br>
              </p><p align="center">
              <a href="#news">News</a> &nbsp;/&nbsp;
              <a href="#publications">Publications</a> &nbsp;/&nbsp;
              <a href="#patents">Patents</a> &nbsp;/&nbsp; 
              <a href="#articles">Articles</a> 
            </p>

            </td>
            <td style="padding:2.5%;width:35%;max-width:35%">
              <a href="images/kartik.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/kartik.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2 id="news">News</h2>
              <h4>
                  [2022]: Created my github page. <br>
                  [2022]: Started on my github page. <br>

              </h4>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2 id="publications">Publications</h2>
              <p>
                Most of my research projects are related to computer vision, optical character recognition, image processing and video processing.
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/easter2.jpg" type="image/jpg">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerf_supervision.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://waymo.com/research/block-nerf/">
                <papertitle>Easter2.0: Improving Convolutional models for Handwritten Text Recognition</papertitle>
              </a>
              <br>
              <strong>Kartik Chaudhary</strong>,  <br>
              <a href="https://scholar.google.com/citations?user=3oLGqC0AAAAJ&hl=en">Raghav Bali</a>, 
              <br>
              <em>Arxiv</em>, 2022  
              <br>
							<a href="https://arxiv.org/pdf/2205.14879.pdf">arXiv</a> /
							<a href="https://github.com/kartikgill/Easter2">code</a> 
              <p></p>
              <p>Easter2.0 is a small/fast convolutional model for the task of OCR/HTR that works well even when labelled data is limited.</p>
            </td>
          </tr>


          <tr>
            <td width="30%" valign="top"><a href="assets/nnclr.png"><img src="assets/nnclr.png" width="100%" style="border-style: none"></a>
            </td><td width="70%" valign="top">
              <p><a href="https://arxiv.org/abs/2104.14548" id="NNCLR">
                <heading>With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations</heading></a><br>
                <b>Debidatta Dwibedi</b>, <a href="https://people.csail.mit.edu/yusuf/">Yusuf Aytar</a>, <a href="https://cims.nyu.edu/~tompson/">Jonathan Tompson</a>, <a href="https://sermanet.github.io/home/">Pierre Sermanet</a>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a> <br>
                <em><a href="https://iccv2021.thecvf.com/home">International Conference on Computer Vision (ICCV) 2021 </a></em>
                <br>
                <br>
                Improve contrastive losses used in self-supervised learning with nearest-neighbors.<br>

              </p>

              <div class="paper" id="nnclr">
                <a href="https://arxiv.org/abs/2104.14548">paper</a> |
                <a href="javascript:toggleblock(&#39;nnclr_abs&#39;)">abstract</a> |
                <a shape="rect" href="javascript:togglebib(&#39;nnclr&#39;)" class="togglebib">bibtex</a>

                <p align="justify"> <i id="nnclr_abs" style="display: none;"> Self-supervised learning algorithms based on instance discrimination train encoders to be invariant to pre-defined transformations of the same instance. While most methods treat different views of the same image as positives for a contrastive loss, we are interested in using positives from other instances in the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), samples the nearest neighbors from the dataset in the latent space, and treats them as positives. This provides more semantic variations than pre-defined transformations. We find that using the nearest-neighbor as positive in contrastive losses improves performance significantly on ImageNet classification, from 71.7% to 75.6%, outperforming previous state-of-the-art methods. On semi-supervised learning benchmarks we improve performance significantly when only 1% ImageNet labels are available, from 53.8% to 56.5%. On transfer learning benchmarks our method outperforms state-of-the-art methods (including supervised learning with ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate empirically that our method is less reliant on complex data augmentations. We see a relative reduction of only 2.1% ImageNet Top-1 accuracy when we train using only random crops.</i></p>

                <div style="white-space: pre-wrap; display: none;" class="bib">
                @InProceedings{Dwibedi_2021_ICCV,
                    author    = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
                    title     = {With a Little Help From My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations},
                    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
                    month     = {October},
                    year      = {2021},
                    pages     = {9588-9597}
                }
             </div>
           </div>
         </td>
       </tr>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="patents">Patents</h2>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody>

  <tr>
    <td width="30%" valign="top"><a href="link"><img src="assets/cuboid2.png" width="100%" style="border-style: none"></a>
    </td><td width="70%" valign="center">
      <p>
        <a href="link"><heading>title</heading></a><br>
      </p>

    </td>
  </tr>

</tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="articles">Articles</h2>
  </td></tr>
</tbody></table>
					

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <br>
    <p align="right"><font size="2">
      <a href="https://www.cs.berkeley.edu/~barron/">this guy's webpage is awesome</a>
    </font></p>

  </td></tr>


</tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
